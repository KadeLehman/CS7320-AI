{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Search: Playing Connect 4\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Total Points: Undegraduates 10, graduate students 11\n",
    "\n",
    "Complete this notebook and submit it. The notebook needs to be a complete project report with your implementation, documentation including a short discussion of how your implementation works and your design choices, and experimental results (e.g., tables and charts with simulation results) with a short discussion of what they mean. Use the provided notebook cells and insert additional code and markdown cells as needed.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "You will implement different versions of agents that play Connect 4:\n",
    "\n",
    "> \"Connect 4 is a two-player connection board game, in which the players choose a color and then take turns dropping colored discs into a seven-column, six-row vertically suspended grid. The pieces fall straight down, occupying the lowest available space within the column. The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one's own discs.\" (see [Connect Four on Wikipedia](https://en.wikipedia.org/wiki/Connect_Four))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Defining the Search Problem [1 point]\n",
    "\n",
    "Define the components of the search problem:\n",
    "\n",
    "* Initial state: an empty board.\n",
    "* Actions: unfilled columns, {0-6} at game start.\n",
    "* Transition model: When a player drops a piece, it falls straight down to the lowest available space in the column.\n",
    "* Goal state: Four of the player's pieces are lined up vertically, horizontally, or diagonally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the 42 squares can have three values (empty, x, or o). However,\n",
    "* pieces cannot float above empty squares\n",
    "* the # of x's is always within one of the # of o's\n",
    "* both players can't have 4-in-a-row at once, etc.\n",
    "\n",
    "So, the search space has significantly less than this number of states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(3**42)\n",
    "print('{:.1e}'.format(3**42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Game Environment and Random Agent [2 point]\n",
    "\n",
    "Implement an agent that plays randomly and let two random agents play against each other 1000 times. How often does each player win? Is the result expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"sources\"\"\"\n",
    "# winning_move() and its kernels -> https://stackoverflow.com/a/63991845\n",
    "# in get_actions_ordered(): removing array elements from another array -> https://stackoverflow.com/a/66695755\n",
    "# show_board() is modified code that M. Hahsler found here -> https://stackoverflow.com/questions/43971138/python-plotting-colored-grid-based-on-values\n",
    "\n",
    "\"\"\"imports\"\"\"\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "\"\"\"rng seeding\"\"\"\n",
    "np.random.seed(0)\n",
    "\n",
    "\"\"\"globals\"\"\"\n",
    "DEBUG = 0\n",
    "HEIGHT = 5\n",
    "WIDTH = 5\n",
    "TURNS = 0\n",
    "\n",
    "\"\"\"kernels for winning_move()\"\"\"\n",
    "horizontal_kernel = np.array([[ 1, 1, 1, 1]])\n",
    "vertical_kernel = np.transpose(horizontal_kernel)\n",
    "diag1_kernel = np.eye(4, dtype=np.uint8)\n",
    "diag2_kernel = np.fliplr(diag1_kernel)\n",
    "detection_kernels = [horizontal_kernel, vertical_kernel, diag1_kernel, diag2_kernel]\n",
    "\n",
    "def show_board(board, fontsize = 10):  \n",
    "    \"\"\"display a board (numpy array)\"\"\"\n",
    "    board = np.copy(board) # make a deep copy first so the original maze is not changed\n",
    "    board = board + np.ones((HEIGHT, WIDTH), np.int8)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    cmap = colors.ListedColormap(['yellow', 'white', 'red'])\n",
    "    ax.imshow(board, cmap = cmap, norm = colors.BoundaryNorm(list(range(cmap.N + 1)), cmap.N))\n",
    "    plt.show()\n",
    "\n",
    "def empty_board(shape=(HEIGHT, WIDTH)):\n",
    "    return np.zeros(shape, np.int8)\n",
    "\n",
    "def winning_move(board, player):\n",
    "    if(player>0):\n",
    "        for kernel in detection_kernels:\n",
    "            if (convolve2d(board>0,kernel,mode=\"valid\")==4).any():\n",
    "                return 1\n",
    "    elif(player<0):\n",
    "        for kernel in detection_kernels:\n",
    "            if (convolve2d(board<0,kernel,mode=\"valid\")==4).any():\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "def check_win(board):\n",
    "    \"\"\"check the board and return one of 1, -1, d (draw), or n (for next move)\"\"\"\n",
    "    \n",
    "    # check for win\n",
    "    if(winning_move(board,1)): return 1\n",
    "    if(winning_move(board,-1)): return -1\n",
    "    \n",
    "    # check for draw\n",
    "    if(np.sum(board==0)<1): return 'd'\n",
    "    \n",
    "    return 'n'\n",
    "\n",
    "def get_actions(board):\n",
    "    \"\"\"Returns non-full columns as a vector of indices\"\"\"\n",
    "    top_row = board[0][0:WIDTH]\n",
    "    if(DEBUG): print(top_row)\n",
    "    actions = np.where(top_row == 0)[0]\n",
    "    # if(DEBUG): print(actions)\n",
    "    return actions\n",
    "\n",
    "def other(player): \n",
    "    if player > 0: return -1\n",
    "    else: return 1\n",
    "\n",
    "def result(state, player, action):\n",
    "    \"\"\"Add move to the board\"\"\"\n",
    "    state = state.copy()\n",
    "    for i in range(HEIGHT):\n",
    "        if(state[HEIGHT-i-1][action] == 0):\n",
    "            state[HEIGHT-i-1][action] = player\n",
    "            return state\n",
    "    sys.exit('error -> column full')\n",
    "\n",
    "def utility(state, player = 1):\n",
    "    \"\"\"check if a state is terminal and return the utility if it is. None means not a terminal mode.\"\"\"\n",
    "    goal = check_win(state)        \n",
    "    if goal == player: return +1 \n",
    "    if goal == 'd': return 0  \n",
    "    if goal == other(player): return -1  # loss is failure\n",
    "    return None # continue\n",
    "\n",
    "def is_terminal(state): \n",
    "    \"\"\"check is a state is a terminal state\"\"\"\n",
    "    return check_win(state) != 'n'\n",
    "\n",
    "def random_player(board, player = None):\n",
    "    \"\"\"Simple player that chooses a random unfilled column.\n",
    "       The function argument named 'player' is unused.\"\"\"\n",
    "    action = np.random.choice(get_actions(board), size=None, replace=False)\n",
    "    # if(DEBUG): print(action)\n",
    "    return action\n",
    "\n",
    "def switch_player(player, r, y):\n",
    "    if player > 0:\n",
    "        return -1, y\n",
    "    else:\n",
    "        return 1, r\n",
    "\n",
    "def play(r, y, N = 1):\n",
    "    global TURNS\n",
    "    results = {'r': 0, 'y': 0, 'd': 0}\n",
    "    for i in range(N):\n",
    "        TURNS = 0\n",
    "        \n",
    "        board = empty_board()\n",
    "        player, fun = 1, r\n",
    "        \n",
    "        while True:\n",
    "            a = fun(board, player)\n",
    "            board = result(board, player, a)\n",
    "            \n",
    "            win = check_win(board)\n",
    "            if win != 'n':\n",
    "                if   win==  1: win='r'\n",
    "                elif win== -1: win='y'\n",
    "                results[win] += 1\n",
    "                break\n",
    "            \n",
    "            player, fun = switch_player(player, r, y)\n",
    "    return results\n",
    "\n",
    "    \n",
    "if(DEBUG>1):\n",
    "    \"\"\"Test check_win\"\"\"\n",
    "    board = empty_board()\n",
    "    for i in range(4): board = result(board, 1, 0)\n",
    "    print(board)\n",
    "    print('Win? ' + check_win(board))\n",
    "    print()\n",
    "    board = empty_board()\n",
    "    for i in range(4): board = result(board, -1, 0)\n",
    "    print(board)\n",
    "    print('Win? ' + check_win(board))\n",
    "    print()\n",
    "    board = empty_board()\n",
    "    print(board)\n",
    "    print('Win? ' + check_win(board))\n",
    "    print()\n",
    "    \n",
    "    \"\"\"Test utility\"\"\"\n",
    "    print(utility(np.full(shape=[HEIGHT, WIDTH], fill_value= 1)))\n",
    "    print(utility(np.full(shape=[HEIGHT, WIDTH], fill_value= -1)))\n",
    "    print(utility(empty_board()))\n",
    "    print()\n",
    "    \n",
    "    \"\"\"Test is_terminal\"\"\"\n",
    "    print(is_terminal(np.full(shape=[HEIGHT, WIDTH], fill_value= 1)))\n",
    "    print(is_terminal(np.full(shape=[HEIGHT, WIDTH], fill_value= -1)))\n",
    "    print(is_terminal(empty_board()))\n",
    "    \n",
    "if(DEBUG<2):\n",
    "    %timeit -n 1 -r 1 display(play(random_player, random_player, N = 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With a standard 6x7 board, player 'x' won 560 times and player 'o' won 437 times.\n",
    "* I expected this result, because player 'x' always goes first. This makes it more likely for player 'x' to win, because they are the first player that is able to win the game (on their fourth turn) and always has more or equal the number of turns taken compared to player 'o'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Minimax Search with Alpha-Beta Pruning [4 points]\n",
    "\n",
    "### Implement the search starting from a given board and specifying the player.\n",
    "\n",
    "__Note:__ The search space for a $6 \\times 7$ board is large. You can experiment with smaller boards (the smallest is $4 \\times 4$) and/or changing the winning rule to connect 3 instead of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"globals\"\"\"\n",
    "DEBUG_ = 0 # 1 ... count nodes, 2 ... debug each node\n",
    "COUNT = 0\n",
    "\n",
    "def get_actions_ordered(board):\n",
    "    \"\"\"return possible actions as a vector of priority-ordered indices\"\"\"\n",
    "    priority = np.array([3,4,2,1,0]) # hard-coded for 7-column board [3,4,2,5,1,6,0]\n",
    "    \n",
    "    all_actions = np.arange(WIDTH)\n",
    "    valid_actions = get_actions(board)\n",
    "    \n",
    "    # get invalid actions\n",
    "    indices = np.argwhere(np.isin(all_actions,valid_actions))\n",
    "    invalid_actions = np.delete(all_actions,indices)\n",
    "    \n",
    "    # remove invalid actions from priority list\n",
    "    indices = np.argwhere(np.isin(priority,invalid_actions))\n",
    "    priority = np.delete(priority,indices)\n",
    "\n",
    "    return priority\n",
    "\n",
    "if(DEBUG_>2):\n",
    "    board = empty_board()   \n",
    "    print(board)\n",
    "    get_actions_ordered(board)\n",
    "\n",
    "def alpha_beta_search(board, player = 1):\n",
    "    \"\"\"start the search.\"\"\"\n",
    "    global DEBUG_,COUNT\n",
    "    COUNT = 0\n",
    "    \n",
    "    value, move = max_value_ab(board, player, -math.inf, +math.inf)\n",
    "    \n",
    "    if DEBUG_ >= 1: print(f\"Number of nodes searched: {COUNT}\") \n",
    "    \n",
    "    return value, move\n",
    "\n",
    "def max_value_ab(state, player, alpha, beta):\n",
    "    \"\"\"player's best move.\"\"\"\n",
    "    global DEBUG_,COUNT\n",
    "    COUNT+=1\n",
    "       \n",
    "    # return utility of state IF it is a terminal state\n",
    "    v = utility(state, player)\n",
    "    if DEBUG_ >= 2:\n",
    "        print(\"max: \")\n",
    "        print( str(state) + str([alpha, beta, v]) )\n",
    "    if v is not None: return v, None\n",
    "        \n",
    "    v, move = -math.inf, None\n",
    "\n",
    "    # check all possible actions in the state, update alpha and return move with the largest value\n",
    "    for a in get_actions_ordered(state):\n",
    "        v2, a2 = min_value_ab(result(state, player, a), player, alpha, beta)\n",
    "        if v2 > v:\n",
    "            v, move = v2, a\n",
    "            alpha = max(alpha, v)\n",
    "        if v >= beta: return v, move\n",
    "    \n",
    "    return v, move\n",
    "\n",
    "def min_value_ab(state, player, alpha, beta):\n",
    "    \"\"\"opponent's best response.\"\"\"\n",
    "    global DEBUG_,COUNT\n",
    "    COUNT+=1\n",
    "    \n",
    "    # return utility of state IF it is a terminal state\n",
    "    v = utility(state, player)\n",
    "    if DEBUG_ >= 2:\n",
    "        print(\"min: \")\n",
    "        print( str(state) + str([alpha, beta, v]) ) \n",
    "    if v is not None: return v, None\n",
    "    \n",
    "    v, move = +math.inf, None\n",
    "\n",
    "    # check all possible actions in the state, update beta and return move with the smallest value\n",
    "    for a in get_actions_ordered(state):\n",
    "        v2, a2 = max_value_ab(result(state, other(player), a), player, alpha, beta)\n",
    "        if v2 < v:\n",
    "            v, move = v2, a\n",
    "            beta = min(beta, v)\n",
    "        if v <= alpha: return v, move\n",
    "    \n",
    "    return v, move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with some manually created boards (at least 5) to check if the agent spots winning opportunities."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# red is about to win (play in column 2)\n",
    "\n",
    "board = np.array([[1,0,0,-1,0,-1,1],\n",
    "                  [-1,1,-1,1,-1,1,-1],\n",
    "                  [1,-1,1,-1,1,1,-1],\n",
    "                  [1,-1,1,-1,1,1,1],\n",
    "                  [1,-1,-1,-1,1,-1,-1],\n",
    "                  [-1,1,-1,1,-1,1,-1]])\n",
    "\n",
    "show_board(board)\n",
    "%timeit -n 1 -r 1 display(alpha_beta_search(board))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# red is about to win (play in column 5)\n",
    "\n",
    "board = np.array([[0,0,0,0,0,0,1],\n",
    "                  [1,-1,-1,-1,1,-1,1],\n",
    "                  [-1,1,-1,1,-1,1,-1],\n",
    "                  [1,-1,1,-1,1,1,-1],\n",
    "                  [1,-1,1,-1,1,1,1],\n",
    "                  [1,-1,-1,-1,1,-1,-1]])\n",
    "\n",
    "show_board(board)\n",
    "%timeit -n 1 -r 1 display(alpha_beta_search(board))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# red can draw if it chooses 0.\n",
    "\n",
    "board = np.array([[0,0,0,0,1,-1,1],\n",
    "                  [1,-1,-1,-1,1,-1,1],\n",
    "                  [-1,1,-1,1,-1,1,-1],\n",
    "                  [1,-1,1,-1,1,1,-1],\n",
    "                  [1,-1,1,-1,1,1,1],\n",
    "                  [1,-1,-1,-1,1,-1,-1]])\n",
    "\n",
    "show_board(board)\n",
    "%timeit -n 1 -r 1 display(alpha_beta_search(board))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# red can win if it chooses 3. Even though red is preventing yellow from winning this turn,\n",
    "# a subtle shift also occurs where red is guaranteed to win since both players play optimally.\n",
    "\n",
    "board = np.array([[0,0,0,0,0,0,0],\n",
    "                  [0,0,0,0,0,-1,0],\n",
    "                  [0,1,-1,1,0,1,0],\n",
    "                  [1,-1,1,-1,0,-1,1],\n",
    "                  [-1,-1,1,-1,1,-1,1],\n",
    "                  [1,1,-1,-1,1,-1,-1]])\n",
    "\n",
    "show_board(board)\n",
    "%timeit -n 1 -r 1 display(alpha_beta_search(board))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# red can win if it chooses 3, because that sets up a trap where red has two winning opportunies at the same time.\n",
    "\n",
    "board = np.array([[0,0,0,0,0,0,0],\n",
    "                  [0,0,0,0,0,1,0],\n",
    "                  [1,-1,0,0,0,-1,1],\n",
    "                  [-1,-1,1,0,1,-1,1],\n",
    "                  [1,1,-1,-1,1,-1,-1],\n",
    "                  [1,-1,1,-1,-1,1,-1]])\n",
    "\n",
    "show_board(board)\n",
    "%timeit -n 1 -r 1 display(alpha_beta_search(board))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns.\n",
    "\n",
    "* For a 6x7 board that is nearly empty, alpha-beta search takes an substantial amount of time to make a move.\n",
    "* With a 6x4 board that is also nearly empty, the search takes much less time to make a move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move ordering\n",
    "\n",
    "Describe and implement a simple move ordering strategy. How does this strategy influence the time it takes to \n",
    "make a move?\n",
    "\n",
    "* My implementation for move ordering is above my alpha-beta search function. The agent prioritizes centralized moves over moves towards the edges of the board. This strategy significantly reduces the time it takes to make a move, because terminal states are reached faster when better moves are considered first that generally allow for more future winning opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playtime\n",
    "\n",
    "Let the Minimax Search agent play a random agent on a small board. Analyze wins, losses and draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG__ = 0\n",
    "\n",
    "def minimax_ab_player(board, player = 1):\n",
    "    \"\"\"Minimax player that makes two moves randomly, then plays based on searching.\"\"\"\n",
    "    global TURNS\n",
    "    TURNS+=1\n",
    "    if TURNS<=8:\n",
    "        if DEBUG__: print(\"random\")\n",
    "        action = np.random.choice(get_actions(board), size=None, replace=False)\n",
    "    else:\n",
    "        utility, action = alpha_beta_search(board, player)\n",
    "        if DEBUG__: print(\"util: \", utility)\n",
    "    if DEBUG__: print(\"chosen: \", action)\n",
    "    return action\n",
    "\n",
    "%timeit -n 1 -r 1 display(play(minimax_ab_player, random_player, N = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Heuristic Alpha-Beta Tree Search [3 points] \n",
    "\n",
    "### Heuristic evaluation function\n",
    "\n",
    "Define and implement a heuristic evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutting off search \n",
    "\n",
    "Modify your Minimax Search with Alpha-Beta Pruning to cut off search at a specified depth and use the heuristic evaluation function. Experiment with different cutoff values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the same manually created boards as above to check if the agent spots wining opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playtime\n",
    "\n",
    "Let two heuristic search agents (different cutoff depth or different heuristic evaluation function) compete against each other on a reasonably sized board. Since there is no randomness, you only need to let them play once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge task [+ 1 bonus point]\n",
    "\n",
    "Find another student and let your best agent play against the other student's best player. We will set up a class tournament on Canvas. This tournament will continue after the submission deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graduate student advanced task: Pure Monte Carlo Search and Best First Move [1 point]\n",
    "\n",
    "__Undergraduate students:__ This is a bonus task you can attempt if you like [+1 Bonus point].\n",
    "\n",
    "### Pure Monte Carlos Search\n",
    "\n",
    "Implement Pure Monte Carlo Search and investigate how this search performs on the test boards that you have used above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best First Move\n",
    "\n",
    "How would you determine what the best first move is? You can use Pure Monte Carlo Search or any algorithms \n",
    "that you have implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
